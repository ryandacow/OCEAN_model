# ğŸ§  OCEAN Personality Trait Predictor

This project predicts personality traits based on free-form text using the **OCEAN model** (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism). It includes:

- A **Flask API** that serves predictions
- A **Jupyter notebook** with training logic and preprocessing
- A **public dataset** for transparency
- A **live demo** on my portfolio site

---

## ğŸŒ Try the Live Demo

Test it here:  
ğŸ‘‰ [ryanneo.vercel.app/personality](https://ryanneo.vercel.app/personality)

---

## ğŸš€ Project Structure

```bash
project-root/
â”œâ”€â”€ models/                # Trained model and vectorizers
â”‚   â”œâ”€â”€ ocean_model.pkl
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚   â”œâ”€â”€ source_ohe.pkl
â”‚   â””â”€â”€ topic_stopwords.pkl
â”œâ”€â”€ datasets/              # Training dataset
â”‚   â””â”€â”€ OCEAN_essays.csv
â”œâ”€â”€ notebooks/             # Training and experimentation
â”‚   â””â”€â”€ OCEAN_Model.ipynb
â”œâ”€â”€ app.py                 # Flask API logic
â”œâ”€â”€ requirements.txt       # Dependencies for API
â”œâ”€â”€ .gitignore
â”œâ”€â”€ render.yaml
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset

The dataset used for training is included in `/datasets/OCEAN_essays.csv`.  
It contains free-text essays labeled with personality traits (`y/n`) across the five OCEAN dimensions.

Additional training data is loaded in the notebook from public datasets via Hugging Face.

---

## ğŸ§  Model Training

All training logic is implemented in [`notebooks/OCEAN_Model.ipynb`](notebooks/OCEAN_Model.ipynb).

Training steps include:

- Text preprocessing:
  - Tokenization
  - Lowercasing
  - Stopword removal (including custom stopwords)
  - Lemmatization using NLTK
- Feature engineering:
  - TF-IDF vectorization of essay text
  - One-hot encoding of essay source
- Multi-label classification using `OneVsRestClassifier` with logistic regression

The trained model and preprocessing objects are saved as `.pkl` files in `/models/`.

---

## ğŸ”§ Flask API

The API is implemented in [`app.py`](app.py). It:

- Accepts POST requests at `/analyze`
- Preprocesses user text using the same pipeline used in training
- Outputs:
  - Rounded probabilities for each trait
  - Binary vector of trait presence
  - Final predicted traits based on thresholds and margin logic

---

### ğŸ”Œ Run Locally

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Run the API:

```bash
python app.py
```

3. Example request:

Send a POST request to `http://localhost:5000/analyze` with a JSON body:

```json
{
  "text": "I love working with people and sharing new ideas.",
  "source": "reddit"
}
```

---

## âš™ï¸ Dependencies

Only the serving dependencies are included in `requirements.txt` (Flask, NumPy, NLTK, etc.).

If you want to run the training notebook, you may need to install additional packages like:

- pandas
- seaborn
- sentence-transformers
- datasets

These are intentionally excluded from the runtime environment for the API to keep it lightweight.

---

## ğŸ“« Contact

Feel free to reach out via [my GitHub profile](https://github.com/ryandacow) with questions or feedback.

---

## ğŸ›¡ License

This project is intended for educational and research use. Attribution is appreciated.
